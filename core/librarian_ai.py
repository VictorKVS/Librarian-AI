# üìÑ –§–∞–π–ª: re_ranker.py
# üìÇ –ü—É—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫–∏: librarian_ai/core/re_ranker.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –ø–æ–≤—Ç–æ—Ä–Ω–æ–µ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ (Re-Ranking) —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
# üì• –ü–æ–ª—É—á–∞–µ—Ç: —Å–ø–∏—Å–æ–∫ –∫–∞–Ω–¥–∏–¥–∞—Ç–æ–≤ –æ—Ç retriever (–≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫)
# ‚öôÔ∏è –î–µ–ª–∞–µ—Ç: –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å —Å –ø–æ–º–æ—â—å—é CrossEncoder (–∏–ª–∏ –º–æ–¥–µ–ª–∏ rerank)
# üì§ –ü–µ—Ä–µ–¥–∞—ë—Ç: –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (–≤ LLM –∏–ª–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é)

# üö® –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è:
# 1. üìò –í—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ CrossEncoder (–Ω–∞–ø—Ä–∏–º–µ—Ä, ruBERT, DeepPavlov –¥–ª—è –†–§)
# 2. üîÑ –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: batched rerank, –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å, –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
# 3. üá∑üá∫ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —á–µ—Ä–µ–∑ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏
# 4. üî¨ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–∏–µ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, —Å—Ç–æ–ø-—Å–ª–æ–≤–∞

from typing import List, Dict
from sentence_transformers import CrossEncoder
import string
from pymorphy2 import MorphAnalyzer

morph = MorphAnalyzer()

class ReRanker:
    def __init__(self, model_name="DeepPavlov/rubert-base-cased-conversational"):
        self.model = CrossEncoder(model_name)

    def lemmatize(self, word: str) -> str:
        """–ü—Ä–∏–≤–æ–¥–∏—Ç —Å–ª–æ–≤–æ –∫ –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º–µ."""
        return morph.parse(word)[0].normal_form

    def preprocess(self, text: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—è, –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ —Ü–∏—Ñ—Ä."""
        words = text.split()
        clean_words = [self.lemmatize(w) for w in words if w.isalpha()]
        return ' '.join(clean_words)

    def rerank(self, query: str, docs: List[Dict], top_k: int = 5) -> List[Dict]:
        """
        –†–∞–Ω–∂–∏—Ä—É–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏.
        :param query: –∏—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        :param docs: —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫–ª—é—á–æ–º 'text'
        :param top_k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        :return: —Å–ø–∏—Å–æ–∫ top_k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        """
        query_clean = self.preprocess(query)
        pairs = [[query_clean, self.preprocess(doc["text"])] for doc in docs]
        scores = self.model.predict(pairs)

        for i, score in enumerate(scores):
            docs[i]["score"] = float(score)

        return sorted(docs, key=lambda x: x["score"], reverse=True)[:top_k]


if __name__ == "__main__":
    reranker = ReRanker()
    query = "–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
    candidates = [
        {"text": "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã —Ö—Ä–∞–Ω–∏—Ç—å—Å—è —Å–æ–≥–ª–∞—Å–Ω–æ 152-–§–ó."},
        {"text": "–®–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ ‚Äî –∫–ª—é—á–µ–≤–æ–π –º–µ—Ö–∞–Ω–∏–∑–º –∑–∞—â–∏—Ç—ã."},
        {"text": "–ü–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤–∫–ª—é—á–∞—é—Ç –§–ò–û, –°–ù–ò–õ–° –∏ –∞–¥—Ä–µ—Å –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è."}
    ]
    top = reranker.rerank(query, candidates)
    for doc in top:
        print(f"‚≠ê {doc['score']:.4f} | {doc['text']}")
