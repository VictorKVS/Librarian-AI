# üìÑ core/tools/loader.py
# üìå –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ: –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤

import os
import asyncio
import logging
from typing import List, Tuple, Optional, Dict, Union
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import lru_cache
from pathlib import Path
import mimetypes
import magic
from tempfile import NamedTemporaryFile
from dataclasses import dataclass
from datetime import datetime
import hashlib  # ‚Üê —ç—Ç–æ –æ—Ç–¥–µ–ª—å–Ω–æ

# –ò–º–ø–æ—Ä—Ç –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª–µ–π —Ç–µ–∫—Å—Ç–∞
from utils.file_utils import (
    extract_text_from_pdf,
    extract_text_from_docx,
    extract_text_from_xlsx,
    extract_text_from_pptx,
    extract_text_from_odf,
    extract_text_from_html,
    extract_text_from_txt,
    extract_text_from_image
)

# –ò–º–ø–æ—Ä—Ç –∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—è –∏–∑ –∞—Ä—Ö–∏–≤–æ–≤
from core.tools.archive_extractors import extract_text_from_archive 

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Å—Ç–∞–Ω—Ç—ã
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50 MB
CACHE_LIMIT = 100
CHUNK_SIZE_DEFAULT = 1000

# –ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö
@dataclass
class FileMetadata:
    name: str
    size: int
    modified: float
    mime_type: str
    checksum: str
    language: str = "en"

@dataclass
class ProcessingResult:
    chunks: List[str]
    metadata: FileMetadata
    processing_time: float

# –ò–Ω–¥–µ–∫—Å MIME-—Ç–∏–ø–æ–≤
SUPPORTED_MIME_TYPES = {
    'application/pdf': 'pdf',
    'application/vnd.openxmlformats-officedocument.wordprocessingml.document': 'docx',
    'application/vnd.openxmlformats-officedocument.presentationml.presentation': 'pptx',
    'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': 'xlsx',
    'application/vnd.oasis.opendocument.text': 'odt',
    'text/html': 'html',
    'text/plain': 'txt',
    'image/jpeg': 'jpg',
    'image/png': 'png'
    # –ê—Ä—Ö–∏–≤—ã
    'application/zip': 'zip',
    'application/x-tar': 'tar',
    'application/x-rar-compressed': 'rar'
}

# –†–∞—Å—á—ë—Ç –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–æ–π —Å—É–º–º—ã
def calculate_checksum(file_path: str) -> str:
    """–í—ã—á–∏—Å–ª—è–µ—Ç SHA-256 —Ö–µ—à —Ñ–∞–π–ª–∞"""
    sha256 = hashlib.sha256()
    with open(file_path, 'rb') as f:
        while chunk := f.read(8192):
            sha256.update(chunk)
    return sha256.hexdigest()

# –û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
class FileLoader:
    def __init__(self, chunker: 'TextChunker', max_workers: int = 4):
        self.chunker = chunker
        self.max_workers = max_workers
        self._cache: Dict[str, ProcessingResult] = {}
        self._cache_hits = 0

    async def load_file(
        self,
        file_path: Union[str, Path],
        chunk_size: int = CHUNK_SIZE_DEFAULT,
        max_chunks: Optional[int] = None,
        language: str = "en"
    ) -> ProcessingResult:
        """
        –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ñ–∞–π–ª
        Args:
            file_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ –≤ —Å–∏–º–≤–æ–ª–∞—Ö
            max_chunks: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤
            language: –Ø–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–Ω–æ–π —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–∏
        Returns:
            ProcessingResult —Å —á–∞–Ω–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        file_path = str(file_path)
        if file_path in self._cache:
            self._cache_hits += 1
            return self._cache[file_path]

        if not self._validate_file(file_path):
            raise FileProcessingError(f"Invalid file: {file_path}")

        start_time = datetime.now().timestamp()
        metadata = await self._get_file_metadata(file_path)
        
        try:
            text = await self._extract_text(file_path, metadata.mime_type)
            chunks = self.chunker.chunk(text, chunk_size, language)
            
            if max_chunks and len(chunks) > max_chunks:
                chunks = chunks[:max_chunks]
                logger.warning(f"Truncated to {max_chunks} chunks for {file_path}")
            
            result = ProcessingResult(
                chunks=chunks,
                metadata=metadata,
                processing_time=datetime.now().timestamp() - start_time
            )
            
            self._cache[file_path] = result
            self.clear_least_used_cache()
            
            return result
            
        except Exception as e:
            logger.error(f"Error processing {file_path}: {str(e)}", exc_info=True)
            raise FileProcessingError(f"Failed to process {file_path}") from e

    async def load_files(
        self,
        file_paths: List[Union[str, Path]],
        chunk_size: int = CHUNK_SIZE_DEFAULT,
        max_workers: Optional[int] = None,
        timeout: int = 300
    ) -> List[ProcessingResult]:
        """
        –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ñ–∞–π–ª–æ–≤
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤
            max_workers: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Ç–æ–∫–æ–≤
            timeout: –¢–∞–π–º–∞—É—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        results = []
        max_workers = max_workers or self.max_workers
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            loop = asyncio.get_event_loop()
            futures = [
                loop.run_in_executor(
                    executor,
                    lambda p=path: asyncio.run(self.load_file(p, chunk_size))
                )
                for path in file_paths
            ]
            
            for future in as_completed(futures, timeout=timeout):
                try:
                    results.append(await future)
                except Exception as e:
                    logger.error(f"File processing failed: {str(e)}")
                    continue
                    
        return results

    async def _extract_text(self, file_path: str, mime_type: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞"""
        file_type = SUPPORTED_MIME_TYPES.get(mime_type)
        if not file_type:
            raise ValueError(f"Unsupported file type: {mime_type}")

        extractors = {
            'pdf': extract_text_from_pdf,
            'docx': extract_text_from_docx,
            'pptx': extract_text_from_pptx,
            'xlsx': extract_text_from_xlsx,
            'odt': extract_text_from_odf,
            'html': extract_text_from_html,
            'txt': extract_text_from_txt,
            'jpg': extract_text_from_image,
            'png': extract_text_from_image,
            'zip': extract_text_from_archive,   # ‚úÖ –î–æ–±–∞–≤–∏—Ç—å
            'rar': extract_text_from_archive,   # ‚úÖ –î–æ–±–∞–≤–∏—Ç—å
            'tar': extract_text_from_archive    # ‚úÖ –î–æ–±–∞–≤–∏—Ç—å
        }
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            lambda: extractors[file_type](file_path)
        )

    async def _get_file_metadata(self, file_path: str) -> FileMetadata:
        """–ü–æ–ª—É—á–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª–∞"""
        path = Path(file_path)
        mime_type, file_type = detect_file_type(file_path)
        
        return FileMetadata(
            name=path.name,
            size=path.stat().st_size,
            modified=path.stat().st_mtime,
            mime_type=mime_type,
            checksum=calculate_checksum(file_path),
            language=self._detect_language(file_path)
        )

    def _validate_file(self, file_path: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ñ–∞–π–ª –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        try:
            path = Path(file_path)
            return all([
                path.exists(),
                path.is_file(),
                path.stat().st_size <= MAX_FILE_SIZE,
                detect_file_type(file_path)[1] != 'unknown'
            ])
        except Exception:
            return False

    def _detect_language(self, file_path: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —è–∑—ã–∫ —Ç–µ–∫—Å—Ç–∞ (—É–ø—Ä–æ—â—ë–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
        # –¢—É—Ç –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é —Å –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π
        return "en"

    def clear_least_used_cache(self):
        """–£–¥–∞–ª—è–µ—Ç –Ω–∞–∏–º–µ–Ω–µ–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–µ –∑–∞–ø–∏—Å–∏ –∏–∑ –∫–µ—à–∞"""
        if len(self._cache) > CACHE_LIMIT:
            sorted_keys = sorted(self._cache.keys(), key=lambda k: self._cache[k].processing_time, reverse=False)
            least_used_key = sorted_keys[0]
            del self._cache[least_used_key]

    def clear_cache(self):
        """–û—á–∏—â–∞–µ—Ç –≤–µ—Å—å –∫–µ—à –∑–∞–≥—Ä—É–∑—á–∏–∫–∞"""
        self._cache.clear()
        self._cache_hits = 0

    @property
    def cache_info(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–µ—à–∞"""
        return {
            'cache_size': len(self._cache),
            'cache_hits': self._cache_hits
        }

# –§—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è MIME-—Ç–∏–ø–∞
def detect_file_type(file_path: str) -> Tuple[str, str]:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç MIME-—Ç–∏–ø –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
    mime = magic.from_file(file_path, mime=True)
    return mime, SUPPORTED_MIME_TYPES.get(mime, 'unknown')

# –ò—Å–∫–ª—é—á–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞
class FileProcessingError(Exception):
    """–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ñ–∞–π–ª–∞"""
    pass

# –§—É–Ω—Ü–∫–∏—è —Ñ–∞–±—Ä–∏–∫–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
def create_file_loader(chunker: 'TextChunker', max_workers: int = 4) -> FileLoader:
    """–°–æ–∑–¥–∞–µ—Ç —ç–∫–∑–µ–º–ø–ª—è—Ä FileLoader"""
    return FileLoader(chunker, max_workers)