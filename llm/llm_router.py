# üìÇ llm/llm_router.py
import os
import logging
import requests
from enum import Enum
from typing import Optional, Literal, Dict, Callable
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class LLMProvider(str, Enum):
    """–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ LLM-–ø—Ä–æ–≤–∞–π–¥–µ—Ä—ã"""
    OPENROUTER = "openrouter"  # –ë–µ—Å—Ü–µ–Ω–∑—É—Ä–Ω—ã–µ –º–æ–¥–µ–ª–∏ (Weaver)
    GIGACHAT = "gigachat"      # –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π (–°–±–µ—Ä)
    YANDEXGPT = "yandexgpt"    # Yandex Cloud LLM
    MISTRAL = "mistral"        # –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å (–±–µ–∑ —Ü–µ–Ω–∑—É—Ä—ã)
    DOLPHIN = "dolphin"        # Dolphin-Mixtral —á–µ—Ä–µ–∑ OpenRouter

class LLMRouter:
    def __init__(self, default_provider: LLMProvider = LLMProvider.GIGACHAT):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞"""
        self.default_provider = default_provider
        self._init_providers()
        
    def _init_providers(self) -> Dict[LLMProvider, Callable]:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
        self.providers = {
            LLMProvider.OPENROUTER: self._query_openrouter,
            LLMProvider.GIGACHAT: self._query_gigachat,
            LLMProvider.YANDEXGPT: self._query_yandexgpt,
            LLMProvider.DOLPHIN: self._query_dolphin,
            LLMProvider.MISTRAL: self._init_mistral()
        }
    
    def generate(
        self,
        prompt: str,
        provider: Optional[LLMProvider] = None,
        **kwargs
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä
        Args:
            prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
            provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default_provider)
            kwargs: –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –º–æ–¥–µ–ª–∏ (temperature, max_tokens –∏ —Ç.–¥.)
        Returns:
            –û—Ç–≤–µ—Ç LLM
        """
        provider = provider or self.default_provider
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä: {provider.value}")
        
        try:
            return self.providers[provider](prompt, **kwargs)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ {provider.value}: {str(e)}")
            return self._fallback(prompt)

    # === –†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ ===

    def _query_openrouter(self, prompt: str, **kwargs) -> str:
        """OpenRouter API (Mancer/Weaver)"""
        headers = {
            "Authorization": f"Bearer {os.getenv('OPENROUTER_KEY')}",
            "HTTP-Referer": "https://librarian.ai",
        }
        data = {
            "model": "mancer/weaver",
            "messages": [{"role": "user", "content": prompt}],
            **kwargs
        }
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=10
        )
        return response.json()["choices"][0]["message"]["content"]

    def _query_gigachat(self, prompt: str, **kwargs) -> str:
        """GigaChat API (–°–±–µ—Ä)"""
        headers = {
            "Authorization": f"Bearer {os.getenv('GIGACHAT_SECRET')}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "GigaChat:latest",
            "messages": [{"role": "user", "content": prompt}],
            **kwargs
        }
        response = requests.post(
            "https://gigachat.devices.sberbank.ru/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=15
        )
        return response.json()["choices"][0]["message"]["content"]

    def _query_yandexgpt(self, prompt: str, **kwargs) -> str:
        """YandexGPT API"""
        headers = {
            "Authorization": f"Api-Key {os.getenv('YANDEX_API_KEY')}",
            "Content-Type": "application/json"
        }
        data = {
            "model": "general",
            "messages": [{"role": "user", "content": prompt}],
            **kwargs
        }
        response = requests.post(
            "https://llm.api.cloud.yandex.net/llm/v1alpha/instruct",
            headers=headers,
            json=data,
            timeout=10
        )
        return response.json()["result"]["alternatives"][0]["message"]["text"]

    def _query_dolphin(self, prompt: str, **kwargs) -> str:
        """Dolphin-Mixtral (—á–µ—Ä–µ–∑ OpenRouter)"""
        headers = {
            "Authorization": f"Bearer {os.getenv('OPENROUTER_KEY')}",
        }
        data = {
            "model": "cognitivecomputations/dolphin-mixtral",
            "messages": [{"role": "user", "content": prompt}],
            "temperature": 0.3,
            **kwargs
        }
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=data,
            timeout=15
        )
        return response.json()["choices"][0]["message"]["content"]

    def _init_mistral(self) -> Callable:
        """–õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–π Mistral"""
        model = None
        tokenizer = None

        def generate(prompt: str, **kwargs) -> str:
            nonlocal model, tokenizer
            if model is None:
                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Mistral...")
                model = AutoModelForCausalLM.from_pretrained(
                    "mistralai/Mistral-7B-Instruct-v0.2",
                    device_map="auto",
                    torch_dtype=torch.float16,
                    load_in_4bit=True
                )
                tokenizer = AutoTokenizer.from_pretrained(
                    "mistralai/Mistral-7B-Instruct-v0.2"
                )
            inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
            outputs = model.generate(**inputs, max_new_tokens=512)
            return tokenizer.decode(outputs[0], skip_special_tokens=True)

        return generate

    def _fallback(self, prompt: str) -> str:
        """–†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ö–∞–Ω–∏–∑–º –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö"""
        logger.warning("–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ä–µ–∑–µ—Ä–≤–Ω—ã–π –ø—Ä–æ–≤–∞–π–¥–µ—Ä (GigaChat)")
        try:
            return self._query_gigachat(prompt)
        except Exception as e:
            logger.critical("–û—à–∏–±–∫–∞ fallback –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞: " + str(e))
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –∑–∞–ø—Ä–æ—Å."

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ç–æ—Ä–∞
default_router = LLMRouter()

def query_llm(
    prompt: str,
    provider: Optional[Literal["openrouter", "gigachat", "yandexgpt", "mistral", "dolphin"]] = None,
    **kwargs
) -> str:
    """
    –£–ø—Ä–æ—â–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
    Args:
        prompt: –¢–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞
        provider: –ò–º—è –ø—Ä–æ–≤–∞–π–¥–µ—Ä–∞ (–µ—Å–ª–∏ None - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è default_router)
        kwargs: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è LLM
    """
    return default_router.generate(
        prompt,
        provider=LLMProvider(provider) if provider else None,
        **kwargs
    )