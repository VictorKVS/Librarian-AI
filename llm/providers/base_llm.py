# ğŸ“„ Ğ¤Ğ°Ğ¹Ğ»: base_llm.py
# ğŸ“‚ ĞŸÑƒÑ‚ÑŒ: librarian_ai/llm/providers/base_llm.py

"""ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ° Ñ‚Ğ¸Ğ¿Ğ¾Ğ² Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (ModelType)

ModelInfo Ğ²ĞºĞ»ÑÑ‡Ğ°ĞµÑ‚ timeout, batch_size, rate_limit, max_retries

ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¼ĞµĞ½ĞµĞ´Ğ¶ĞµÑ€ (async with)

ĞÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ¸ Ğ¿Ğ¾Ñ‚Ğ¾ĞºĞ¾Ğ²Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ (stream)

ĞĞ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ°Ñ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²

Ğ“Ñ€ĞµĞ¹ÑÑ„ÑƒĞ» Ğ´ĞµĞ³Ñ€Ğ°Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¸ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°Ñ…

ğŸ§  Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾ Ğº Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ SmartRouter, QualityAnalyzer, MemoryAugmenter, Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ°Ğ¼Ğ¸ (yandex_gpt.py, deepseek.py, Ğ¸ Ğ´"""





from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Union, AsyncGenerator
from dataclasses import dataclass
from enum import Enum, auto
import logging
import asyncio
from contextlib import asynccontextmanager
import numpy as np

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """Ğ¢Ğ¸Ğ¿Ñ‹ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ñ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"""
    CHAT = auto()
    INSTRUCT = auto()
    EMBEDDING = auto()
    MULTIMODAL = auto()
    CODE = auto()

@dataclass(frozen=True)
class ModelInfo:
    """ĞĞµĞ¸Ğ·Ğ¼ĞµĞ½ÑĞµĞ¼Ñ‹Ğµ Ğ¼ĞµÑ‚Ğ°Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸"""
    name: str
    provider: str
    model_type: ModelType = ModelType.CHAT
    context_length: int = 8192
    supports_streaming: bool = False
    supports_embeddings: bool = False
    supports_vision: bool = False
    version: str = "1.0"
    description: Optional[str] = None
    cost_per_token: Optional[float] = None
    max_retries: int = 3
    timeout: float = 30.0
    batch_size: Optional[int] = None
    rate_limit_qps: Optional[int] = None
    concurrency_limit: Optional[int] = None

class BaseLLM(ABC):
    """
    Ğ‘Ğ°Ğ·Ğ¾Ğ²Ñ‹Ğ¹ ĞºĞ»Ğ°ÑÑ Ğ´Ğ»Ñ Ğ²ÑĞµÑ… LLM-Ğ¿Ñ€Ğ¾Ğ²Ğ°Ğ¹Ğ´ĞµÑ€Ğ¾Ğ²
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or {}
        self._model_info = self._initialize_model_info()
        self._initialized = False
        self._session = None

    @property
    def model_info(self) -> ModelInfo:
        return self._model_info

    @abstractmethod
    def _initialize_model_info(self) -> ModelInfo:
        """Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ñ‚ Ğ¾Ğ±ÑŠĞµĞºÑ‚ ModelInfo"""
        raise NotImplementedError

    async def initialize(self) -> None:
        if not self._initialized:
            try:
                await self._async_init()
                self._initialized = True
                logger.info(f"[{self}] Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½")
            except Exception as e:
                logger.exception(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¸Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ {self}: {e}")
                raise

    @abstractmethod
    async def _async_init(self) -> None:
        pass

    @abstractmethod
    async def generate_async(
        self,
        prompt: str,
        *,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        **kwargs
    ) -> str:
        pass

    async def stream(
        self,
        prompt: str,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        try:
            if self.model_info.supports_streaming:
                async for chunk in self._stream_impl(prompt, **kwargs):
                    yield chunk
            else:
                response = await self.generate_async(prompt, **kwargs)
                yield response
        except Exception as e:
            logger.exception(f"[{self}] ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¿Ñ€Ğ¸ stream: {e}")
            yield f"[ĞÑˆĞ¸Ğ±ĞºĞ°] {str(e)}"

    @abstractmethod
    async def _stream_impl(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        raise NotImplementedError

    def generate(self, prompt: str, **kwargs) -> str:
        try:
            return asyncio.run(self.generate_async(prompt, **kwargs))
        except Exception as e:
            logger.error(f"[{self}] Ğ¡Ğ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ°Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ñ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ: {str(e)}")
            return f"[ĞÑˆĞ¸Ğ±ĞºĞ°] {str(e)}"

    async def get_embeddings(self, texts: List[str]) -> List[List[float]]:
        if not self.model_info.supports_embeddings:
            raise NotImplementedError("Ğ­Ñ‚Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸")
        vectors = await self._get_embeddings_impl(texts)
        return self._normalize_embeddings(vectors)

    @abstractmethod
    async def _get_embeddings_impl(self, texts: List[str]) -> List[List[float]]:
        raise NotImplementedError

    def _normalize_embeddings(self, vectors: List[List[float]]) -> List[List[float]]:
        try:
            array = np.array(vectors)
            norm = np.linalg.norm(array, axis=1, keepdims=True)
            norm[norm == 0] = 1e-8
            normalized = array / norm
            return normalized.tolist()
        except Exception as e:
            logger.warning(f"ĞĞµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¸: {str(e)}")
            return vectors

    async def is_available(self) -> bool:
        for attempt in range(self.model_info.max_retries):
            try:
                return await self._check_availability()
            except Exception as e:
                logger.warning(f"[{self}] ĞŸĞ¾Ğ¿Ñ‹Ñ‚ĞºĞ° #{attempt+1} Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ°ÑÑŒ: {e}")
                await asyncio.sleep(1)
        return False

    async def _check_availability(self) -> bool:
        return True

    async def close(self) -> None:
        if self._initialized:
            try:
                await self._async_close()
                self._initialized = False
                logger.info(f"[{self}] ĞºĞ¾Ñ€Ñ€ĞµĞºÑ‚Ğ½Ğ¾ Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚")
            except Exception as e:
                logger.error(f"[{self}] Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°ĞºÑ€Ñ‹Ñ‚Ğ¸Ñ: {e}")

    async def _async_close(self) -> None:
        pass

    @asynccontextmanager
    async def context(self):
        await self.initialize()
        try:
            yield self
        finally:
            await self.close()

    async def __aenter__(self):
        await self.initialize()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    def __str__(self) -> str:
        return f"{self.model_info.provider}:{self.model_info.name}"

    def __repr__(self) -> str:
        return (f"<{self.__class__.__name__} "
                f"provider={self.model_info.provider} "
                f"name={self.model_info.name} "
                f"type={self.model_info.model_type.name} "
                f"v{self.model_info.version}>")
