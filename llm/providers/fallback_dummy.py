# ðŸ“„ Ð¤Ð°Ð¹Ð»: fallback_dummy.py
# ðŸ“‚ ÐŸÑƒÑ‚ÑŒ: librarian_ai/llm/providers/fallback_dummy.py

import asyncio
import logging
from typing import List, AsyncGenerator
from .base_llm import BaseLLM, ModelInfo, ModelType

logger = logging.getLogger(__name__)

class FallbackDummyProvider(BaseLLM):
    def _initialize_model_info(self) -> ModelInfo:
        return ModelInfo(
            name="DummyLLM",
            provider="fallback",
            model_type=ModelType.CHAT,
            context_length=2048,
            supports_streaming=False,
            supports_embeddings=False,
            version="0.1",
            description="Ð ÐµÐ·ÐµÑ€Ð²Ð½Ð°Ñ Ð·Ð°Ð³Ð»ÑƒÑˆÐºÐ° Ð´Ð»Ñ Ð¾Ñ‚ÐºÐ°Ð·Ð¾ÑƒÑÑ‚Ð¾Ð¹Ñ‡Ð¸Ð²Ð¾ÑÑ‚Ð¸"
        )

    async def _async_init(self):
        await asyncio.sleep(0.1)  # Ð­Ð¼ÑƒÐ»ÑÑ†Ð¸Ñ Ð¸Ð½Ð¸Ñ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸

    async def generate_async(self, prompt: str, **kwargs) -> str:
        logger.warning(f"âš ï¸ DummyLLM Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ÑÑ Ð²Ð¼ÐµÑÑ‚Ð¾ Ð½Ð°ÑÑ‚Ð¾ÑÑ‰ÐµÐ¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°: {prompt[:50]}...")
        return "ðŸ” [Ð—Ð°Ð³Ð»ÑƒÑˆÐºÐ° Ð¾Ñ‚Ð²ÐµÑ‚Ð° Ð¾Ñ‚ DummyLLM â€” LLM ÑÐµÐ¹Ñ‡Ð°Ñ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½]"

    async def _stream_impl(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        yield await self.generate_async(prompt, **kwargs)

    async def _get_embeddings_impl(self, texts: List[str]) -> List[List[float]]:
        return [[0.0] * 768 for _ in texts]  # Ð¤Ð¸ÐºÑ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð²ÐµÐºÑ‚Ð¾Ñ€Ñ‹

    async def _check_availability(self) -> bool:
        return True