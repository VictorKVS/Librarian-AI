# ðŸ“„ Ð¤Ð°Ð¹Ð»: local_model.py
# ðŸ“‚ ÐŸÑƒÑ‚ÑŒ: librarian_ai/llm/local_model.py
""""
Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚ÑŒ: pip install transformers torch

ÐœÐ¾Ð¶Ð½Ð¾ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ Ð»ÑŽÐ±ÑƒÑŽ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼ÑƒÑŽ Ð¼Ð¾Ð´ÐµÐ»ÑŒ: gpt2, EleutherAI/gpt-neo-1.3B, NousResearch/llama-2-7b-hf Ð¸ Ñ‚.Ð´.

Ð Ð°Ð±Ð¾Ñ‚Ð°ÐµÑ‚ Ð² ÑÐ²ÑÐ·ÐºÐµ Ñ llm_router.py"""

import asyncio
import logging
from typing import List, AsyncGenerator
from .providers.base_llm import BaseLLM, ModelInfo, ModelType

try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
except ImportError:
    AutoTokenizer = None
    AutoModelForCausalLM = None

logger = logging.getLogger(__name__)

class LocalLLM(BaseLLM):
    def _initialize_model_info(self) -> ModelInfo:
        return ModelInfo(
            name="LocalModel",
            provider="local",
            model_type=ModelType.CHAT,
            context_length=2048,
            supports_streaming=False,
            supports_embeddings=False,
            version="0.1",
            description="Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð°Ñ LLM Ð½Ð° Ð±Ð°Ð·Ðµ transformers"
        )

    async def _async_init(self):
        if not AutoTokenizer or not AutoModelForCausalLM:
            raise ImportError("ðŸ¤– ÐÐµ ÑƒÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ñ‹ transformers. pip install transformers torch")

        model_path = self.config.get("model_path", "gpt2")
        device = self.config.get("device", "cpu")

        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        self.model = AutoModelForCausalLM.from_pretrained(model_path)
        self.model.to(device)
        self.device = device

        logger.info(f"ðŸ§  Ð›Ð¾ÐºÐ°Ð»ÑŒÐ½Ð°Ñ Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð°: {model_path} [{device}]")

    async def generate_async(self, prompt: str, max_tokens: int = 200, **kwargs) -> str:
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        output = self.model.generate(
            **inputs,
            max_new_tokens=max_tokens,
            do_sample=kwargs.get("temperature", 0.7) > 0,
            temperature=kwargs.get("temperature", 0.7),
        )
        result = self.tokenizer.decode(output[0], skip_special_tokens=True)
        return result[len(prompt):].strip()

    async def _stream_impl(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        response = await self.generate_async(prompt, **kwargs)
        yield response

    async def _get_embeddings_impl(self, texts: List[str]) -> List[List[float]]:
        raise NotImplementedError("Ð­Ð¼Ð±ÐµÐ´Ð´Ð¸Ð½Ð³Ð¸ Ð½Ðµ Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÑŽÑ‚ÑÑ Ð² Ð±Ð°Ð·Ð¾Ð²Ð¾Ð¹ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ð¾Ð¹ Ð¼Ð¾Ð´ÐµÐ»Ð¸")

    async def _check_availability(self) -> bool:
        return self.model is not None